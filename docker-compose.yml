services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - bigdata-net

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - bigdata-net

  redis:
    image: redis:7
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - bigdata-net

  prediction-sink:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: prediction-sink
    command: python src/storage/prediction_sink.py
    volumes:
      - ./src:/app/src
      - ./models:/app/models
      - ./data:/app/data
    depends_on:
      - kafka
      - redis
    networks:
      - bigdata-net

  minio:
    image: minio/minio
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio-data:/data
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: admin123
    command: server /data --console-address ":9001"
    networks:
      - bigdata-net

  airflow:
    build:
      context: .                      # Quan trọng: Context là root để thấy requirements.txt
      dockerfile: airflow/Dockerfile.airflow  # Đường dẫn tới file Dockerfile mới
    container_name: airflow
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__CORE__FERNET_KEY: "kx1JAY9FV_NfQFQKDR_pSP-L1tNR1S2iUlkUhFmg6zU="
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      PROJECT_DIR: /app
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./:/app
    command: bash -c "airflow db migrate && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true && airflow scheduler & exec airflow webserver"
    ports:
      - "8080:8080"
    networks:
      - bigdata-net

  api:
    build: .
    container_name: api-service
    command: uvicorn src.api.app:app --host 0.0.0.0 --port 8000
    volumes:
      - ./src:/app/src
      - ./models:/app/models
      - ./data:/app/data
    ports:
      - "8000:8000"
    networks:
      - bigdata-net

  consumer:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: consumer-worker
    command: python src/streaming/consumer_worker.py
    volumes:
      - ./src:/app/src
      - ./models:/app/models
      - ./data:/app/data
    depends_on:
      - kafka
      - redis
    networks:
      - bigdata-net

  generator:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: data-generator
    command: python src/ingestion/data_generator.py
    volumes:
      - ./src:/app/src
      - ./models:/app/models
      - ./data:/app/data
    depends_on:
      - kafka
    networks:
      - bigdata-net

  dashboard:
    build: .
    container_name: dashboard
    command: streamlit run src/dashboard/app.py --server.port 8501 --server.address 0.0.0.0
    volumes:
      - ./src:/app/src
      - ./models:/app/models
      - ./data:/app/data
    ports:
      - "8501:8501"
    depends_on:
      - redis
      - prediction-sink
    networks:
      - bigdata-net

volumes:
  redis-data:
  minio-data:

networks:
  bigdata-net:
